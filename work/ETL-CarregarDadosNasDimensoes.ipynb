{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import das bibliotecas\n",
    "import pyspark\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import collections\n",
    "import os\n",
    "from os.path import isfile, isdir, join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--jars file:///home/jovyan/jdbc/postgresql-42.2.17.jar pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_host = \"161.97.71.108\"\n",
    "db_port = \"15432\"\n",
    "db_name = os.getenv('POSTGRES_DB')\n",
    "db_user = os.getenv('POSTGRES_USER')\n",
    "db_pass = os.getenv('POSTGRES_PASSWORD')\n",
    "db_driver = \"org.postgresql.Driver\"\n",
    "db_url = \"jdbc:postgresql://\"+db_host+\":\"+db_port+\"/\" + db_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quando for True, as tabelas das dimensões serão recriadas e carregadas\n",
    "carregar_dimensoes = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inicialização do spark\n",
    "conf = SparkConf() \\\n",
    "        .setMaster(\"local[2]\") \\\n",
    "        .setAppName(\"LendoDB\") \\\n",
    "        .set(\"spark.executor.memory\", \"4g\") \\\n",
    "        .set(\"spark.driver.memory\", \"4g\") \\\n",
    "        .set(\"spark.driver.maxResultSize\", \"2g\") \\\n",
    "        .set(\"spark.ui.enabled\", \"true\") \\\n",
    "        .set(\"spark.sql.shuffle.partitions\" , \"800\") \\\n",
    "        .set(\"spark.sql.execution.arrow.pyspark.enabled\" , \"false\") \\\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .config(conf=conf) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definindo o schema dos dados para leitura dos arquivos JSON\n",
    "schema = StructType([\n",
    "    StructField(\"dadosBasicos\", StructType([\n",
    "        StructField(\"assunto\", ArrayType(\n",
    "            StructType([\n",
    "                StructField(\"assuntoLocal\", StructType([\n",
    "                    StructField(\"codigoAssunto\", LongType(), True),\n",
    "                    StructField(\"codigoPaiNacional\", LongType(), True),\n",
    "                    StructField(\"descricao\", StringType(), True)\n",
    "                ]), True),\n",
    "                StructField(\"codigoNacional\", LongType(), True),\n",
    "                StructField(\"principal\", BooleanType(), True)\n",
    "            ]),\n",
    "        ), True),\n",
    "        StructField('classeProcessual', LongType(), True),\n",
    "        StructField('codigoLocalidade', StringType(), True),\n",
    "        StructField('competencia', StringType(), True),\n",
    "        StructField('dataAjuizamento', StringType(), True),\n",
    "        StructField('dscSistema', StringType(), True),\n",
    "        StructField('nivelSigilo', LongType(), True),\n",
    "        StructField('numero', StringType(), True),\n",
    "        StructField(\"orgaoJulgador\", StructType([\n",
    "            StructField(\"codigoMunicipioIBGE\", LongType(), True),\n",
    "            StructField(\"codigoOrgao\", StringType(), True),\n",
    "            StructField(\"instancia\", StringType(), True),\n",
    "            StructField(\"nomeOrgao\", StringType(), True)\n",
    "        ]), True),\n",
    "        StructField('procEl', LongType(), True),\n",
    "        StructField(\"tamanhoProcesso\", StringType(), True),\n",
    "        StructField(\"totalAssuntos\", LongType(), True),\n",
    "        StructField(\"valorCausa\", StringType(), True)       \n",
    "    ]), True),\n",
    "    StructField(\"grau\", StringType(), True),\n",
    "    StructField(\"millisInsercao\", LongType(), True),\n",
    "    StructField(\"movimento\", ArrayType(     \n",
    "        StructType([\n",
    "            StructField(\"complementoNacional\", ArrayType(\n",
    "                StructType([\n",
    "                    StructField(\"codComplemento\", LongType(), True),\n",
    "                    StructField(\"codComplementoTabelado\", LongType(), True),\n",
    "                    StructField(\"descricaoComplemento\", StringType(), True),\n",
    "                ])\n",
    "            ), True),\n",
    "            StructField(\"dataHora\", StringType(), True),\n",
    "            StructField(\"idDocumentoVinculado\", ArrayType(\n",
    "                StringType(),\n",
    "            ), True),\n",
    "            StructField(\"identificadorMovimento\", StringType(), True),\n",
    "            StructField(\"movimentoLocal\", StructType([\n",
    "                StructField('codigoMovimento', LongType(), True),\n",
    "                StructField('codigoPaiNacional', LongType(), True)\n",
    "            ]), True),\n",
    "            StructField(\"movimentoNacional\", StructType([\n",
    "                StructField('codigoNacional', LongType(), True)\n",
    "            ]), True),\n",
    "            StructField(\"nivelSigilo\", StringType(), True),\n",
    "            StructField(\"orgaoJulgador\", StructType([\n",
    "                StructField(\"codigoMunicipioIBGE\", LongType(), True),\n",
    "                StructField(\"codigoOrgao\", StringType(), True),\n",
    "                StructField(\"instancia\", StringType(), True),\n",
    "                StructField(\"nomeOrgao\", StringType(), True)\n",
    "            ]), True),\n",
    "            StructField(\"tipoDecisao\", StringType(), True),\n",
    "            StructField(\"tipoResponsavelMovimento\", StringType(), True)\n",
    "        ]),\n",
    "    ), True),\n",
    "    StructField(\"siglaTribunal\", StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tabela inovacnj.classe criada.\n"
     ]
    }
   ],
   "source": [
    "# carrega o CSV de classes e faz a carga da dimensão\n",
    "df_classes = spark.read \\\n",
    "    .option(\"header\",\"true\") \\\n",
    "    .option(\"inferSchema\",\"true\") \\\n",
    "    .option(\"delimiter\",\";\") \\\n",
    "    .csv(\"./base/sgt_classes.csv\")\n",
    "\n",
    "df_classes.createOrReplaceTempView(\"classes\")\n",
    "   \n",
    "df_qry_classes = spark.sql(\n",
    "    \"SELECT \" +\n",
    "    \"codigo AS cod,\" + \n",
    "    \"descricao,\" + \n",
    "    \"sigla,\" + \n",
    "    \"cod_pai AS codpai \" +    \n",
    "    \"FROM classes \"\n",
    ")\n",
    "\n",
    "if carregar_dimensoes :\n",
    "    df_qry_classes.write \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .format(\"jdbc\") \\\n",
    "        .option(\"url\", db_url).option(\"user\", db_user).option(\"password\", db_pass).option(\"driver\", db_driver) \\\n",
    "        .option(\"dbtable\", \"inovacnj.classe\") \\\n",
    "        .save()\n",
    "\n",
    "print(\"tabela inovacnj.classe criada.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tabela inovacnj.assunto criada.\n"
     ]
    }
   ],
   "source": [
    "# carrega o CSV de assuntos e faz a carga da dimensão\n",
    "df_assuntos = spark.read \\\n",
    "    .option(\"header\",\"true\") \\\n",
    "    .option(\"inferSchema\",\"true\") \\\n",
    "    .option(\"delimiter\",\";\") \\\n",
    "    .csv(\"./base/sgt_assuntos.csv\")\n",
    "\n",
    "df_assuntos.createOrReplaceTempView(\"assuntos\")\n",
    "   \n",
    "df_qry_assuntos = spark.sql(\n",
    "    \"SELECT \" +\n",
    "    \"codigo AS cod,\" + \n",
    "    \"descricao,\" + \n",
    "    \"cod_pai AS codpai \" +    \n",
    "    \"FROM assuntos \"\n",
    ")\n",
    "\n",
    "if carregar_dimensoes :\n",
    "    df_qry_assuntos.write \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .format(\"jdbc\") \\\n",
    "        .option(\"url\", db_url).option(\"user\", db_user).option(\"password\", db_pass).option(\"driver\", db_driver) \\\n",
    "        .option(\"dbtable\", \"inovacnj.assunto\") \\\n",
    "        .save()\n",
    "\n",
    "print(\"tabela inovacnj.assunto criada.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tabela inovacnj.movimentocnj criada.\n"
     ]
    }
   ],
   "source": [
    "# carrega o CSV de movimentos e faz a carga da dimensão\n",
    "df_movimentos = spark.read \\\n",
    "    .option(\"header\",\"true\") \\\n",
    "    .option(\"inferSchema\",\"true\") \\\n",
    "    .option(\"delimiter\",\";\") \\\n",
    "    .csv(\"./base/sgt_movimentos.csv\")\n",
    "\n",
    "# cria uma view temporaria dos movimentos\n",
    "df_movimentos.createOrReplaceTempView(\"movimentos\")\n",
    "\n",
    "# carrega o dataframe de movimentos nacionais (com nossa classificacao de fases e natureza)\n",
    "df_movimentosNac = spark.read \\\n",
    "    .option(\"header\",\"true\") \\\n",
    "    .option(\"inferSchema\",\"true\") \\\n",
    "    .option(\"delimiter\",\";\") \\\n",
    "    .csv(\"./base/MovimentosNacionais.csv\")\n",
    "\n",
    "df_movimentosNac.createOrReplaceTempView(\"movimentos_nac\")\n",
    "\n",
    "df_qry_movimentosNac = spark.sql(\n",
    "    \"SELECT \" +\n",
    "    \"trim(substring_index(MOVIMENTO, '-', 1)) AS codmovimento, \" + \n",
    "    \"trim(substring_index(MOVIMENTO, '-', -1)) AS descmovimento, \" + \n",
    "    \"CASE WHEN NATUREZA IS NULL THEN 'GERAL' ELSE NATUREZA END AS natureza, \" +\n",
    "    \"CASE WHEN FASE IS NULL THEN 'F0 - NÃO CLASSIFICADO' ELSE FASE END AS fase \" +\n",
    "    \"FROM movimentos_nac \" +\n",
    "    \"WHERE RELEVANCIA = 1\"\n",
    ")\n",
    "\n",
    "df_movimentos_join = df_movimentos \\\n",
    "    .join(df_qry_movimentosNac, df_movimentos[\"codigo\"] == df_qry_movimentosNac[\"codmovimento\"], \"left\")\n",
    "\n",
    "df_movimentos_join.createOrReplaceTempView(\"movimentos_com_fase\")\n",
    "\n",
    "df_qry_movimentos = spark.sql(\n",
    "    \"SELECT \" +\n",
    "    \"codigo AS cod,\" + \n",
    "    \"descricao,\" + \n",
    "    \"natureza, \" +\n",
    "    \"fase, \" +\n",
    "    \"cod_pai AS codpai \" +    \n",
    "    \"FROM movimentos_com_fase \"\n",
    ")\n",
    "\n",
    "if carregar_dimensoes :\n",
    "    df_qry_movimentos.write \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .format(\"jdbc\") \\\n",
    "        .option(\"url\", db_url).option(\"user\", db_user).option(\"password\", db_pass).option(\"driver\", db_driver) \\\n",
    "        .option(\"dbtable\", \"inovacnj.movimentocnj\") \\\n",
    "        .save()\n",
    "\n",
    "print(\"tabela inovacnj.movimentocnj criada.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tabela inovacnj.tribunal criada.\n"
     ]
    }
   ],
   "source": [
    "# carrega o CSV de tribunal e faz a carga da dimensão\n",
    "df_tribunais = spark.read \\\n",
    "    .option(\"header\",\"true\") \\\n",
    "    .option(\"inferSchema\",\"true\") \\\n",
    "    .option(\"delimiter\",\",\") \\\n",
    "    .csv(\"./base/tribunal.csv\")\n",
    "\n",
    "df_tribunais.createOrReplaceTempView(\"tribunais\")\n",
    "\n",
    "df_qry_df_tribunais = spark.sql(\n",
    "    \"SELECT * \" +\n",
    "    \"FROM tribunais \"\n",
    ")\n",
    "\n",
    "if carregar_dimensoes :\n",
    "    df_qry_df_tribunais.write \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .format(\"jdbc\") \\\n",
    "        .option(\"url\", db_url).option(\"user\", db_user).option(\"password\", db_pass).option(\"driver\", db_driver) \\\n",
    "        .option(\"dbtable\", \"inovacnj.tribunal\") \\\n",
    "        .save()\n",
    "\n",
    "print(\"tabela inovacnj.tribunal criada.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tabela inovacnj.orgao_julgador criada.\n"
     ]
    }
   ],
   "source": [
    "# carrega o CSV de serventia e faz a carga da dimensão\n",
    "df_serventias = spark.read \\\n",
    "    .option(\"header\",\"true\") \\\n",
    "    .option(\"inferSchema\",\"true\") \\\n",
    "    .option(\"delimiter\",\";\") \\\n",
    "    .csv(\"./base/mpm_serventias.csv\")\n",
    "\n",
    "df_serventias.createOrReplaceTempView(\"serventias\")\n",
    "\n",
    "df_qry_serventias = spark.sql(\n",
    "    \"SELECT \" +\n",
    "    \"SEQ_ORGAO AS cod, \" + \n",
    "    \"DSC_ORGAO AS descricao, \" + \n",
    "    \"SEQ_ORGAO_PAI AS codpai, \" + \n",
    "    \"TIP_ORGAO AS sigla_tipoj, \" + \n",
    "    \"DSC_TIP_ORGAO AS tipo_oj, \" + \n",
    "    \"DSC_CIDADE AS cidade, \" + \n",
    "    \"SIG_UF AS uf, \" + \n",
    "    \"COD_IBGE AS codibge, \" + \n",
    "    \"TIP_ESFERA_JUSTICA AS esfera \" + \n",
    "    \"FROM serventias \"\n",
    ")\n",
    "\n",
    "if carregar_dimensoes :\n",
    "    df_qry_serventias.write \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .format(\"jdbc\") \\\n",
    "        .option(\"url\", db_url).option(\"user\", db_user).option(\"password\", db_pass).option(\"driver\", db_driver) \\\n",
    "        .option(\"dbtable\", \"inovacnj.orgao_julgador\") \\\n",
    "        .save()\n",
    "\n",
    "print(\"tabela inovacnj.orgao_julgador criada.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_qry_classes = df_qry_classes.withColumnRenamed(\"descricao\", \"descclasse\")\n",
    "df_qry_movimentos = df_qry_movimentos.withColumnRenamed(\"descricao\", \"descmovimento\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando carregamento do ramo de justica: ./base/justica_militar\n",
      "Iniciando carregamento do tribunal: ./base/justica_militar/processos-tjmmg\n",
      "Carregando dataframe do arquivo: ./base/justica_militar/processos-tjmmg/processos-tjmmg_2.json\n",
      "Carregando dataframe do arquivo: ./base/justica_militar/processos-tjmmg/processos-tjmmg_3.json\n",
      "Carregando dataframe do arquivo: ./base/justica_militar/processos-tjmmg/processos-tjmmg_1.json\n",
      "Finalizando carregamento do tribunal: ./base/justica_militar/processos-tjmmg\n",
      "Iniciando carregamento do tribunal: ./base/justica_militar/processos-tjmsp\n",
      "Carregando dataframe do arquivo: ./base/justica_militar/processos-tjmsp/processos-tjmsp_2.json\n",
      "Carregando dataframe do arquivo: ./base/justica_militar/processos-tjmsp/processos-tjmsp_3.json\n",
      "Finalizando carregamento do tribunal: ./base/justica_militar/processos-tjmsp\n",
      "Iniciando carregamento do tribunal: ./base/justica_militar/processos-tjmrs\n",
      "Carregando dataframe do arquivo: ./base/justica_militar/processos-tjmrs/processos-tjmrs_2.json\n",
      "Carregando dataframe do arquivo: ./base/justica_militar/processos-tjmrs/processos-tjmrs_3.json\n",
      "Carregando dataframe do arquivo: ./base/justica_militar/processos-tjmrs/processos-tjmrs_1.json\n",
      "Finalizando carregamento do tribunal: ./base/justica_militar/processos-tjmrs\n",
      "Finalizando carregamento do ramo de justica: ./base/justica_militar\n",
      "Iniciando carregamento do ramo de justica: ./base/justica_trabalho\n",
      "Iniciando carregamento do tribunal: ./base/justica_trabalho/processos-trt24\n",
      "Carregando dataframe do arquivo: ./base/justica_trabalho/processos-trt24/processos-trt24_3.json\n",
      "Carregando dataframe do arquivo: ./base/justica_trabalho/processos-trt24/processos-trt24_2.json\n",
      "Carregando dataframe do arquivo: ./base/justica_trabalho/processos-trt24/processos-trt24_5.json\n",
      "Carregando dataframe do arquivo: ./base/justica_trabalho/processos-trt24/processos-trt24_4.json\n",
      "Carregando dataframe do arquivo: ./base/justica_trabalho/processos-trt24/processos-trt24_6.json\n",
      "Carregando dataframe do arquivo: ./base/justica_trabalho/processos-trt24/processos-trt24_1.json\n",
      "Finalizando carregamento do tribunal: ./base/justica_trabalho/processos-trt24\n",
      "Iniciando carregamento do tribunal: ./base/justica_trabalho/processos-trt23\n",
      "Carregando dataframe do arquivo: ./base/justica_trabalho/processos-trt23/processos-trt23_2.json\n",
      "Finalizando carregamento do tribunal: ./base/justica_trabalho/processos-trt23\n",
      "Iniciando carregamento do tribunal: ./base/justica_trabalho/processos-trt15\n",
      "Carregando dataframe do arquivo: ./base/justica_trabalho/processos-trt15/processos-trt15_1.json\n",
      "Carregando dataframe do arquivo: ./base/justica_trabalho/processos-trt15/processos-trt15_6.json\n",
      "Carregando dataframe do arquivo: ./base/justica_trabalho/processos-trt15/processos-trt15_5.json\n",
      "Carregando dataframe do arquivo: ./base/justica_trabalho/processos-trt15/processos-trt15_4.json\n",
      "Carregando dataframe do arquivo: ./base/justica_trabalho/processos-trt15/processos-trt15_3.json\n",
      "Carregando dataframe do arquivo: ./base/justica_trabalho/processos-trt15/processos-trt15_2.json\n",
      "Finalizando carregamento do tribunal: ./base/justica_trabalho/processos-trt15\n",
      "Iniciando carregamento do tribunal: ./base/justica_trabalho/processos-trt12\n",
      "Carregando dataframe do arquivo: ./base/justica_trabalho/processos-trt12/processos-trt12_3.json\n",
      "Carregando dataframe do arquivo: ./base/justica_trabalho/processos-trt12/processos-trt12_2.json\n",
      "Carregando dataframe do arquivo: ./base/justica_trabalho/processos-trt12/processos-trt12_5.json\n",
      "Carregando dataframe do arquivo: ./base/justica_trabalho/processos-trt12/processos-trt12_4.json\n",
      "Carregando dataframe do arquivo: ./base/justica_trabalho/processos-trt12/processos-trt12_6.json\n",
      "Carregando dataframe do arquivo: ./base/justica_trabalho/processos-trt12/processos-trt12_1.json\n",
      "Finalizando carregamento do tribunal: ./base/justica_trabalho/processos-trt12\n",
      "Iniciando carregamento do tribunal: ./base/justica_trabalho/processos-trt13\n",
      "Carregando dataframe do arquivo: ./base/justica_trabalho/processos-trt13/processos-trt13_6.json\n",
      "Carregando dataframe do arquivo: ./base/justica_trabalho/processos-trt13/processos-trt13_1.json\n",
      "Carregando dataframe do arquivo: ./base/justica_trabalho/processos-trt13/processos-trt13_3.json\n",
      "Carregando dataframe do arquivo: ./base/justica_trabalho/processos-trt13/processos-trt13_2.json\n",
      "Carregando dataframe do arquivo: ./base/justica_trabalho/processos-trt13/processos-trt13_5.json\n",
      "Carregando dataframe do arquivo: ./base/justica_trabalho/processos-trt13/processos-trt13_4.json\n",
      "Finalizando carregamento do tribunal: ./base/justica_trabalho/processos-trt13\n",
      "Iniciando carregamento do tribunal: ./base/justica_trabalho/processos-trt14\n",
      "Carregando dataframe do arquivo: ./base/justica_trabalho/processos-trt14/processos-trt14_5.json\n",
      "Carregando dataframe do arquivo: ./base/justica_trabalho/processos-trt14/processos-trt14_4.json\n",
      "Carregando dataframe do arquivo: ./base/justica_trabalho/processos-trt14/processos-trt14_3.json\n",
      "Carregando dataframe do arquivo: ./base/justica_trabalho/processos-trt14/processos-trt14_2.json\n",
      "Carregando dataframe do arquivo: ./base/justica_trabalho/processos-trt14/processos-trt14_1.json\n",
      "Carregando dataframe do arquivo: ./base/justica_trabalho/processos-trt14/processos-trt14_6.json\n",
      "Finalizando carregamento do tribunal: ./base/justica_trabalho/processos-trt14\n",
      "Iniciando carregamento do tribunal: ./base/justica_trabalho/processos-trt22\n",
      "Carregando dataframe do arquivo: ./base/justica_trabalho/processos-trt22/processos-trt22_5.json\n",
      "Carregando dataframe do arquivo: ./base/justica_trabalho/processos-trt22/processos-trt22_4.json\n",
      "Carregando dataframe do arquivo: ./base/justica_trabalho/processos-trt22/processos-trt22_3.json\n",
      "Carregando dataframe do arquivo: ./base/justica_trabalho/processos-trt22/processos-trt22_2.json\n",
      "Carregando dataframe do arquivo: ./base/justica_trabalho/processos-trt22/processos-trt22_1.json\n",
      "Carregando dataframe do arquivo: ./base/justica_trabalho/processos-trt22/processos-trt22_6.json\n",
      "Finalizando carregamento do tribunal: ./base/justica_trabalho/processos-trt22\n",
      "Iniciando carregamento do tribunal: ./base/justica_trabalho/processos-trt9\n",
      "Carregando dataframe do arquivo: ./base/justica_trabalho/processos-trt9/processos-trt9_5.json\n",
      "Carregando dataframe do arquivo: ./base/justica_trabalho/processos-trt9/processos-trt9_4.json\n",
      "Carregando dataframe do arquivo: ./base/justica_trabalho/processos-trt9/processos-trt9_3.json\n",
      "Carregando dataframe do arquivo: ./base/justica_trabalho/processos-trt9/processos-trt9_2.json\n",
      "Carregando dataframe do arquivo: ./base/justica_trabalho/processos-trt9/processos-trt9_1.json\n",
      "Carregando dataframe do arquivo: ./base/justica_trabalho/processos-trt9/processos-trt9_6.json\n",
      "Finalizando carregamento do tribunal: ./base/justica_trabalho/processos-trt9\n",
      "Iniciando carregamento do tribunal: ./base/justica_trabalho/processos-trt7\n",
      "Carregando dataframe do arquivo: ./base/justica_trabalho/processos-trt7/processos-trt7_4.json\n",
      "Carregando dataframe do arquivo: ./base/justica_trabalho/processos-trt7/processos-trt7_5.json\n",
      "Carregando dataframe do arquivo: ./base/justica_trabalho/processos-trt7/processos-trt7_2.json\n",
      "Carregando dataframe do arquivo: ./base/justica_trabalho/processos-trt7/processos-trt7_3.json\n",
      "Carregando dataframe do arquivo: ./base/justica_trabalho/processos-trt7/processos-trt7_1.json\n",
      "Carregando dataframe do arquivo: ./base/justica_trabalho/processos-trt7/processos-trt7_6.json\n",
      "Finalizando carregamento do tribunal: ./base/justica_trabalho/processos-trt7\n",
      "Iniciando carregamento do tribunal: ./base/justica_trabalho/processos-trt6\n",
      "Carregando dataframe do arquivo: ./base/justica_trabalho/processos-trt6/processos-trt6_1.json\n",
      "Carregando dataframe do arquivo: ./base/justica_trabalho/processos-trt6/processos-trt6_6.json\n",
      "Carregando dataframe do arquivo: ./base/justica_trabalho/processos-trt6/processos-trt6_4.json\n",
      "Carregando dataframe do arquivo: ./base/justica_trabalho/processos-trt6/processos-trt6_5.json\n",
      "Carregando dataframe do arquivo: ./base/justica_trabalho/processos-trt6/processos-trt6_2.json\n",
      "Carregando dataframe do arquivo: ./base/justica_trabalho/processos-trt6/processos-trt6_3.json\n",
      "Finalizando carregamento do tribunal: ./base/justica_trabalho/processos-trt6\n",
      "Iniciando carregamento do tribunal: ./base/justica_trabalho/processos-trt1\n",
      "Carregando dataframe do arquivo: ./base/justica_trabalho/processos-trt1/processos-trt1_2.json\n",
      "Carregando dataframe do arquivo: ./base/justica_trabalho/processos-trt1/processos-trt1_3.json\n",
      "Carregando dataframe do arquivo: ./base/justica_trabalho/processos-trt1/processos-trt1_4.json\n",
      "Carregando dataframe do arquivo: ./base/justica_trabalho/processos-trt1/processos-trt1_5.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carregando dataframe do arquivo: ./base/justica_trabalho/processos-trt1/processos-trt1_6.json\n",
      "Carregando dataframe do arquivo: ./base/justica_trabalho/processos-trt1/processos-trt1_1.json\n",
      "Finalizando carregamento do tribunal: ./base/justica_trabalho/processos-trt1\n",
      "Iniciando carregamento do tribunal: ./base/justica_trabalho/processos-trt8\n",
      "Carregando dataframe do arquivo: ./base/justica_trabalho/processos-trt8/processos-trt8_1.json\n",
      "Carregando dataframe do arquivo: ./base/justica_trabalho/processos-trt8/processos-trt8_6.json\n",
      "Carregando dataframe do arquivo: ./base/justica_trabalho/processos-trt8/processos-trt8_5.json\n",
      "Carregando dataframe do arquivo: ./base/justica_trabalho/processos-trt8/processos-trt8_4.json\n",
      "Carregando dataframe do arquivo: ./base/justica_trabalho/processos-trt8/processos-trt8_3.json\n",
      "Carregando dataframe do arquivo: ./base/justica_trabalho/processos-trt8/processos-trt8_2.json\n",
      "Finalizando carregamento do tribunal: ./base/justica_trabalho/processos-trt8\n",
      "Iniciando carregamento do tribunal: ./base/justica_trabalho/processos-trt20\n",
      "Carregando dataframe do arquivo: ./base/justica_trabalho/processos-trt20/processos-trt20_4.json\n",
      "Carregando dataframe do arquivo: ./base/justica_trabalho/processos-trt20/processos-trt20_5.json\n",
      "Carregando dataframe do arquivo: ./base/justica_trabalho/processos-trt20/processos-trt20_2.json\n",
      "Carregando dataframe do arquivo: ./base/justica_trabalho/processos-trt20/processos-trt20_3.json\n",
      "Carregando dataframe do arquivo: ./base/justica_trabalho/processos-trt20/processos-trt20_1.json\n",
      "Carregando dataframe do arquivo: ./base/justica_trabalho/processos-trt20/processos-trt20_6.json\n",
      "Finalizando carregamento do tribunal: ./base/justica_trabalho/processos-trt20\n",
      "Iniciando carregamento do tribunal: ./base/justica_trabalho/processos-trt18\n",
      "Carregando dataframe do arquivo: ./base/justica_trabalho/processos-trt18/processos-trt18_5.json\n",
      "Carregando dataframe do arquivo: ./base/justica_trabalho/processos-trt18/processos-trt18_4.json\n",
      "Carregando dataframe do arquivo: ./base/justica_trabalho/processos-trt18/processos-trt18_3.json\n",
      "Carregando dataframe do arquivo: ./base/justica_trabalho/processos-trt18/processos-trt18_2.json\n",
      "Carregando dataframe do arquivo: ./base/justica_trabalho/processos-trt18/processos-trt18_1.json\n",
      "Carregando dataframe do arquivo: ./base/justica_trabalho/processos-trt18/processos-trt18_6.json\n",
      "Finalizando carregamento do tribunal: ./base/justica_trabalho/processos-trt18\n",
      "Iniciando carregamento do tribunal: ./base/justica_trabalho/processos-trt11\n",
      "Carregando dataframe do arquivo: ./base/justica_trabalho/processos-trt11/processos-trt11_6.json\n",
      "Carregando dataframe do arquivo: ./base/justica_trabalho/processos-trt11/processos-trt11_1.json\n",
      "Carregando dataframe do arquivo: ./base/justica_trabalho/processos-trt11/processos-trt11_2.json\n",
      "Carregando dataframe do arquivo: ./base/justica_trabalho/processos-trt11/processos-trt11_3.json\n",
      "Carregando dataframe do arquivo: ./base/justica_trabalho/processos-trt11/processos-trt11_4.json\n",
      "Carregando dataframe do arquivo: ./base/justica_trabalho/processos-trt11/processos-trt11_5.json\n",
      "Finalizando carregamento do tribunal: ./base/justica_trabalho/processos-trt11\n",
      "Iniciando carregamento do tribunal: ./base/justica_trabalho/processos-trt16\n",
      "Carregando dataframe do arquivo: ./base/justica_trabalho/processos-trt16/processos-trt16_4.json\n",
      "Carregando dataframe do arquivo: ./base/justica_trabalho/processos-trt16/processos-trt16_5.json\n",
      "Carregando dataframe do arquivo: ./base/justica_trabalho/processos-trt16/processos-trt16_2.json\n",
      "Carregando dataframe do arquivo: ./base/justica_trabalho/processos-trt16/processos-trt16_3.json\n",
      "Carregando dataframe do arquivo: ./base/justica_trabalho/processos-trt16/processos-trt16_1.json\n",
      "Carregando dataframe do arquivo: ./base/justica_trabalho/processos-trt16/processos-trt16_6.json\n",
      "Finalizando carregamento do tribunal: ./base/justica_trabalho/processos-trt16\n",
      "Iniciando carregamento do tribunal: ./base/justica_trabalho/processos-trt17\n",
      "Carregando dataframe do arquivo: ./base/justica_trabalho/processos-trt17/processos-trt17_1.json\n",
      "Carregando dataframe do arquivo: ./base/justica_trabalho/processos-trt17/processos-trt17_6.json\n",
      "Carregando dataframe do arquivo: ./base/justica_trabalho/processos-trt17/processos-trt17_4.json\n",
      "Carregando dataframe do arquivo: ./base/justica_trabalho/processos-trt17/processos-trt17_5.json\n",
      "Carregando dataframe do arquivo: ./base/justica_trabalho/processos-trt17/processos-trt17_2.json\n",
      "Carregando dataframe do arquivo: ./base/justica_trabalho/processos-trt17/processos-trt17_3.json\n",
      "Finalizando carregamento do tribunal: ./base/justica_trabalho/processos-trt17\n",
      "Iniciando carregamento do tribunal: ./base/justica_trabalho/processos-trt10\n",
      "Carregando dataframe do arquivo: ./base/justica_trabalho/processos-trt10/processos-trt10_2.json\n",
      "Carregando dataframe do arquivo: ./base/justica_trabalho/processos-trt10/processos-trt10_3.json\n",
      "Carregando dataframe do arquivo: ./base/justica_trabalho/processos-trt10/processos-trt10_4.json\n",
      "Carregando dataframe do arquivo: ./base/justica_trabalho/processos-trt10/processos-trt10_5.json\n",
      "Carregando dataframe do arquivo: ./base/justica_trabalho/processos-trt10/processos-trt10_6.json\n",
      "Carregando dataframe do arquivo: ./base/justica_trabalho/processos-trt10/processos-trt10_1.json\n",
      "Finalizando carregamento do tribunal: ./base/justica_trabalho/processos-trt10\n",
      "Iniciando carregamento do tribunal: ./base/justica_trabalho/processos-trt19\n",
      "Carregando dataframe do arquivo: ./base/justica_trabalho/processos-trt19/processos-trt19_1.json\n",
      "Carregando dataframe do arquivo: ./base/justica_trabalho/processos-trt19/processos-trt19_6.json\n",
      "Carregando dataframe do arquivo: ./base/justica_trabalho/processos-trt19/processos-trt19_5.json\n",
      "Carregando dataframe do arquivo: ./base/justica_trabalho/processos-trt19/processos-trt19_4.json\n",
      "Carregando dataframe do arquivo: ./base/justica_trabalho/processos-trt19/processos-trt19_3.json\n",
      "Carregando dataframe do arquivo: ./base/justica_trabalho/processos-trt19/processos-trt19_2.json\n",
      "Finalizando carregamento do tribunal: ./base/justica_trabalho/processos-trt19\n",
      "Iniciando carregamento do tribunal: ./base/justica_trabalho/processos-trt21\n",
      "Carregando dataframe do arquivo: ./base/justica_trabalho/processos-trt21/processos-trt21_1.json\n",
      "Carregando dataframe do arquivo: ./base/justica_trabalho/processos-trt21/processos-trt21_6.json\n",
      "Carregando dataframe do arquivo: ./base/justica_trabalho/processos-trt21/processos-trt21_4.json\n",
      "Carregando dataframe do arquivo: ./base/justica_trabalho/processos-trt21/processos-trt21_5.json\n",
      "Carregando dataframe do arquivo: ./base/justica_trabalho/processos-trt21/processos-trt21_2.json\n",
      "Carregando dataframe do arquivo: ./base/justica_trabalho/processos-trt21/processos-trt21_3.json\n",
      "Finalizando carregamento do tribunal: ./base/justica_trabalho/processos-trt21\n",
      "Iniciando carregamento do tribunal: ./base/justica_trabalho/processos-trt4\n",
      "Carregando dataframe do arquivo: ./base/justica_trabalho/processos-trt4/processos-trt4_1.json\n",
      "Carregando dataframe do arquivo: ./base/justica_trabalho/processos-trt4/processos-trt4_6.json\n",
      "Carregando dataframe do arquivo: ./base/justica_trabalho/processos-trt4/processos-trt4_5.json\n",
      "Carregando dataframe do arquivo: ./base/justica_trabalho/processos-trt4/processos-trt4_4.json\n",
      "Carregando dataframe do arquivo: ./base/justica_trabalho/processos-trt4/processos-trt4_3.json\n",
      "Carregando dataframe do arquivo: ./base/justica_trabalho/processos-trt4/processos-trt4_2.json\n",
      "Finalizando carregamento do tribunal: ./base/justica_trabalho/processos-trt4\n",
      "Iniciando carregamento do tribunal: ./base/justica_trabalho/processos-trt3\n",
      "Carregando dataframe do arquivo: ./base/justica_trabalho/processos-trt3/processos-trt3_3.json\n",
      "Carregando dataframe do arquivo: ./base/justica_trabalho/processos-trt3/processos-trt3_2.json\n",
      "Carregando dataframe do arquivo: ./base/justica_trabalho/processos-trt3/processos-trt3_5.json\n",
      "Carregando dataframe do arquivo: ./base/justica_trabalho/processos-trt3/processos-trt3_4.json\n",
      "Carregando dataframe do arquivo: ./base/justica_trabalho/processos-trt3/processos-trt3_6.json\n",
      "Carregando dataframe do arquivo: ./base/justica_trabalho/processos-trt3/processos-trt3_1.json\n",
      "Finalizando carregamento do tribunal: ./base/justica_trabalho/processos-trt3\n",
      "Iniciando carregamento do tribunal: ./base/justica_trabalho/processos-trt2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carregando dataframe do arquivo: ./base/justica_trabalho/processos-trt2/processos-trt2_6.json\n",
      "Carregando dataframe do arquivo: ./base/justica_trabalho/processos-trt2/processos-trt2_1.json\n",
      "Carregando dataframe do arquivo: ./base/justica_trabalho/processos-trt2/processos-trt2_3.json\n",
      "Carregando dataframe do arquivo: ./base/justica_trabalho/processos-trt2/processos-trt2_2.json\n",
      "Carregando dataframe do arquivo: ./base/justica_trabalho/processos-trt2/processos-trt2_5.json\n",
      "Carregando dataframe do arquivo: ./base/justica_trabalho/processos-trt2/processos-trt2_4.json\n",
      "Finalizando carregamento do tribunal: ./base/justica_trabalho/processos-trt2\n",
      "Iniciando carregamento do tribunal: ./base/justica_trabalho/processos-trt5\n",
      "Carregando dataframe do arquivo: ./base/justica_trabalho/processos-trt5/processos-trt5_5.json\n",
      "Carregando dataframe do arquivo: ./base/justica_trabalho/processos-trt5/processos-trt5_4.json\n",
      "Carregando dataframe do arquivo: ./base/justica_trabalho/processos-trt5/processos-trt5_3.json\n",
      "Carregando dataframe do arquivo: ./base/justica_trabalho/processos-trt5/processos-trt5_2.json\n",
      "Carregando dataframe do arquivo: ./base/justica_trabalho/processos-trt5/processos-trt5_1.json\n",
      "Carregando dataframe do arquivo: ./base/justica_trabalho/processos-trt5/processos-trt5_6.json\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o10409.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 11 in stage 676.0 failed 1 times, most recent failure: Lost task 11.0 in stage 676.0 (TID 137656, f4bbee4c98b6, executor driver): java.lang.OutOfMemoryError: Java heap space\n\tat java.base/java.util.Arrays.copyOf(Arrays.java:3745)\n\tat org.apache.hadoop.io.Text.setCapacity(Text.java:266)\n\tat org.apache.hadoop.io.Text.append(Text.java:236)\n\tat org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:245)\n\tat org.apache.hadoop.util.LineReader.readLine(LineReader.java:176)\n\tat org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)\n\tat org.apache.hadoop.mapreduce.lib.input.LineRecordReader.skipUtfByteOrderMark(LineRecordReader.java:152)\n\tat org.apache.hadoop.mapreduce.lib.input.LineRecordReader.nextKeyValue(LineRecordReader.java:192)\n\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)\n\tat org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.hasNext(HadoopFileLinesReader.scala:69)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:93)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:173)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:93)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n\tat scala.collection.Iterator$ConcatIterator.hasNext(Iterator.scala:222)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n\tat scala.collection.Iterator$ConcatIterator.hasNext(Iterator.scala:222)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage8.agg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage8.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.shuffle.sort.UnsafeShuffleWriter.write(UnsafeShuffleWriter.java:177)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2008)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2007)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2007)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:973)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:973)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:973)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2239)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2188)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2177)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:775)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2120)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2139)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2164)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1004)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1003)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:385)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:3450)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3618)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:764)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3616)\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3447)\n\tat jdk.internal.reflect.GeneratedMethodAccessor110.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\nCaused by: java.lang.OutOfMemoryError: Java heap space\n\tat java.base/java.util.Arrays.copyOf(Arrays.java:3745)\n\tat org.apache.hadoop.io.Text.setCapacity(Text.java:266)\n\tat org.apache.hadoop.io.Text.append(Text.java:236)\n\tat org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:245)\n\tat org.apache.hadoop.util.LineReader.readLine(LineReader.java:176)\n\tat org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)\n\tat org.apache.hadoop.mapreduce.lib.input.LineRecordReader.skipUtfByteOrderMark(LineRecordReader.java:152)\n\tat org.apache.hadoop.mapreduce.lib.input.LineRecordReader.nextKeyValue(LineRecordReader.java:192)\n\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)\n\tat org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.hasNext(HadoopFileLinesReader.scala:69)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:93)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:173)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:93)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n\tat scala.collection.Iterator$ConcatIterator.hasNext(Iterator.scala:222)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n\tat scala.collection.Iterator$ConcatIterator.hasNext(Iterator.scala:222)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage8.agg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage8.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.shuffle.sort.UnsafeShuffleWriter.write(UnsafeShuffleWriter.java:177)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-40ed202c3ea4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    100\u001b[0m         )\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mdf_query_distinctPd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_movimentos_join\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoPandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m         \u001b[0mdf_query_distinctPd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./output/movimentos_tribunais.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'a'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_first\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\";\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/pandas/conversion.py\u001b[0m in \u001b[0;36mtoPandas\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0;31m# Below is toPandas without Arrow optimization.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m         \u001b[0mpdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_records\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m         \u001b[0mcolumn_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    594\u001b[0m         \"\"\"\n\u001b[1;32m    595\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectToPython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    597\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBatchedSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPickleSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o10409.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 11 in stage 676.0 failed 1 times, most recent failure: Lost task 11.0 in stage 676.0 (TID 137656, f4bbee4c98b6, executor driver): java.lang.OutOfMemoryError: Java heap space\n\tat java.base/java.util.Arrays.copyOf(Arrays.java:3745)\n\tat org.apache.hadoop.io.Text.setCapacity(Text.java:266)\n\tat org.apache.hadoop.io.Text.append(Text.java:236)\n\tat org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:245)\n\tat org.apache.hadoop.util.LineReader.readLine(LineReader.java:176)\n\tat org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)\n\tat org.apache.hadoop.mapreduce.lib.input.LineRecordReader.skipUtfByteOrderMark(LineRecordReader.java:152)\n\tat org.apache.hadoop.mapreduce.lib.input.LineRecordReader.nextKeyValue(LineRecordReader.java:192)\n\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)\n\tat org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.hasNext(HadoopFileLinesReader.scala:69)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:93)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:173)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:93)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n\tat scala.collection.Iterator$ConcatIterator.hasNext(Iterator.scala:222)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n\tat scala.collection.Iterator$ConcatIterator.hasNext(Iterator.scala:222)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage8.agg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage8.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.shuffle.sort.UnsafeShuffleWriter.write(UnsafeShuffleWriter.java:177)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2008)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2007)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2007)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:973)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:973)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:973)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2239)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2188)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2177)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:775)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2120)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2139)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2164)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1004)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1003)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:385)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:3450)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3618)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:764)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3616)\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3447)\n\tat jdk.internal.reflect.GeneratedMethodAccessor110.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\nCaused by: java.lang.OutOfMemoryError: Java heap space\n\tat java.base/java.util.Arrays.copyOf(Arrays.java:3745)\n\tat org.apache.hadoop.io.Text.setCapacity(Text.java:266)\n\tat org.apache.hadoop.io.Text.append(Text.java:236)\n\tat org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:245)\n\tat org.apache.hadoop.util.LineReader.readLine(LineReader.java:176)\n\tat org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)\n\tat org.apache.hadoop.mapreduce.lib.input.LineRecordReader.skipUtfByteOrderMark(LineRecordReader.java:152)\n\tat org.apache.hadoop.mapreduce.lib.input.LineRecordReader.nextKeyValue(LineRecordReader.java:192)\n\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)\n\tat org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.hasNext(HadoopFileLinesReader.scala:69)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:93)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:173)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:93)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n\tat scala.collection.Iterator$ConcatIterator.hasNext(Iterator.scala:222)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n\tat scala.collection.Iterator$ConcatIterator.hasNext(Iterator.scala:222)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage8.agg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage8.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.shuffle.sort.UnsafeShuffleWriter.write(UnsafeShuffleWriter.java:177)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception happened during processing of request from ('127.0.0.1', 60114)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.8/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/opt/conda/lib/python3.8/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/opt/conda/lib/python3.8/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/opt/conda/lib/python3.8/socketserver.py\", line 720, in __init__\n",
      "    self.handle()\n",
      "  File \"/usr/local/spark/python/pyspark/accumulators.py\", line 268, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/usr/local/spark/python/pyspark/accumulators.py\", line 241, in poll\n",
      "    if func():\n",
      "  File \"/usr/local/spark/python/pyspark/accumulators.py\", line 245, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "  File \"/usr/local/spark/python/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# faz o carregamento de todos os arquivos em um único DataFrame,\n",
    "# geracao do CSV com os dados consolidados\n",
    "# cria a tabela fato com os movimentos processuais\n",
    "\n",
    "basedir = \"./base\"\n",
    "\n",
    "dirs_ramos_justica = [join(basedir, f) for f in os.listdir(basedir) if isdir(join(basedir, f))]\n",
    "\n",
    "is_first = True\n",
    "\n",
    "for dir_ramo_just in dirs_ramos_justica:\n",
    "    print(\"Iniciando carregamento do ramo de justica: \" + dir_ramo_just)\n",
    "    dirs_tribunais = [join(dir_ramo_just, f) for f in os.listdir(dir_ramo_just) if isdir(join(dir_ramo_just, f))]\n",
    "    \n",
    "    for dir_trib in dirs_tribunais:\n",
    "        print(\"Iniciando carregamento do tribunal: \" + dir_trib)\n",
    "        \n",
    "        arquivos = [join(dir_trib, f) for f in os.listdir(dir_trib) if isfile(join(dir_trib, f))]\n",
    "        \n",
    "        df_union_tribunal = spark.createDataFrame(spark.sparkContext.emptyRDD(), schema)\n",
    "        \n",
    "        for arq in arquivos:\n",
    "            if arq.endswith(\".DS_Store\") :\n",
    "                continue\n",
    "                \n",
    "            print(\"Carregando dataframe do arquivo: \" + arq)\n",
    "            df = spark.read.schema(schema).json(arq)\n",
    "            df_union_tribunal = df_union_tribunal.union(df)\n",
    "        \n",
    "        # Cria uma view temporaria para o dataframe\n",
    "        df_union_tribunal.createOrReplaceTempView(\"proc_movimentos\")\n",
    "        \n",
    "        # Query para formato em CSV\n",
    "        df_query_distinct = spark.sql(\n",
    "            \"SELECT DISTINCT \" + \n",
    "            \"siglaTribunal AS codtribunal, \" + \n",
    "            \"grau, \" +\n",
    "            \"millisinsercao, \" +\n",
    "\n",
    "            \"dadosBasicos.classeProcessual AS codclasse, \" +\n",
    "            \"dadosBasicos.codigoLocalidade AS codlocalidade, \" +\n",
    "            \"dadosBasicos.competencia, \" +\n",
    "            \"to_timestamp(dadosBasicos.dataAjuizamento, 'yyyyMMddHHmmss') AS dtajuizamento, \"\n",
    "            \"dadosBasicos.dscSistema AS descsistema, \" +\n",
    "            \"dadosBasicos.nivelSigilo AS nivelsigilo, \" +\n",
    "            \"dadosBasicos.numero AS npu, \" + \n",
    "            \"dadosBasicos.orgaoJulgador.codigoMunicipioIBGE AS oj_codibge, \" +\n",
    "            \"dadosBasicos.orgaoJulgador.codigoOrgao AS oj_cod, \" +\n",
    "            \"dadosBasicos.orgaoJulgador.instancia AS oj_instancia, \" +\n",
    "            \"dadosBasicos.orgaoJulgador.nomeOrgao AS oj_descricao, \" +\n",
    "            \"dadosBasicos.procEl AS tramitacao, \" +\n",
    "            \"dadosBasicos.tamanhoProcesso AS tamanhoprocesso, \" +\n",
    "            \"dadosBasicos.valorCausa AS valorcausa, \" +\n",
    "\n",
    "            \"exp_assunto.assunto.codigoNacional AS ass_cod, \" +\n",
    "            \"exp_assunto.assunto.principal AS ass_principal, \" + \n",
    "            \"exp_assunto.assunto.assuntoLocal.codigoAssunto AS ass_codlocal, \" +\n",
    "            \"exp_assunto.assunto.assuntoLocal.codigoPaiNacional AS ass_codpainacional, \" +\n",
    "            \"exp_assunto.assunto.assuntoLocal.descricao AS ass_desclocal, \" +\n",
    "            \n",
    "            \"exp_movimento.movimento.dataHora AS mov_dtmov, \" +\n",
    "            #\"to_timestamp(exp_movimento.movimento.dataHora, 'yyyyMMddHHmmss') AS mov_dtmov, \" +\n",
    "            \"exp_movimento.movimento.movimentoLocal.codigoMovimento AS mov_codlocal, \" +\n",
    "            \"exp_movimento.movimento.movimentoLocal.codigoPaiNacional AS mov_codpainacional, \" +\n",
    "            \"exp_movimento.movimento.movimentoNacional.codigoNacional AS mov_cod, \" +\n",
    "            \"exp_movimento.movimento.nivelSigilo AS mov_nivelsigilo, \" +\n",
    "\n",
    "            \"exp_movimento.movimento.orgaoJulgador.codigoMunicipioIBGE as mov_oj_codibge, \" +\n",
    "            \"exp_movimento.movimento.orgaoJulgador.codigoOrgao as mov_oj_cod, \" +\n",
    "            \"exp_movimento.movimento.orgaoJulgador.instancia as mov_oj_instancia, \" +\n",
    "            \"exp_movimento.movimento.orgaoJulgador.nomeOrgao as mov_oj_descricao, \" +\n",
    "\n",
    "            \"exp_movimento.movimento.tipoDecisao as mov_tpdecisao, \" +\n",
    "            \"exp_movimento.movimento.tipoResponsavelMovimento as mov_tprespmov \" +\n",
    "\n",
    "            \"FROM proc_movimentos \" + \n",
    "            \"LATERAL VIEW explode(dadosBasicos.assunto) exp_assunto as assunto \" +\n",
    "            \"LATERAL VIEW explode(movimento) exp_movimento as movimento \" + \n",
    "            \"WHERE cast(substring(dadosBasicos.dataAjuizamento,0,4) as INT) >= 2000 AND to_timestamp(dadosBasicos.dataAjuizamento, 'yyyyMMddHHmmss') >= to_timestamp('20000101000000', 'yyyyMMddHHmmss') \" + \n",
    "            \"AND exp_movimento.movimento.movimentoNacional.codigoNacional NOT IN(581, 85, 12270, 12271) \" + \n",
    "            \"AND size(proc_movimentos.movimento) > 0 \"\n",
    "            \"AND (proc_movimentos.movimento[0].movimentoNacional.codigoNacional IN (26, 12474) \" +\n",
    "            \"AND proc_movimentos.movimento[size(proc_movimentos.movimento) -1].movimentoNacional.codigoNacional IN (22, 246)) \"\n",
    "        )\n",
    "        \n",
    "        df_movimentos_join = df_query_distinct \\\n",
    "           .join(df_qry_movimentos, df_query_distinct[\"mov_cod\"] == df_qry_movimentos[\"cod\"], \"left\") \\\n",
    "           .join(df_qry_classes, df_query_distinct[\"codclasse\"] == df_qry_classes[\"cod\"], \"left\") \\\n",
    "           .join(df_qry_assuntos, df_query_distinct[\"ass_cod\"] == df_qry_assuntos[\"cod\"], \"left\") \\\n",
    "           .select( \\\n",
    "                col(\"codtribunal\"), col(\"grau\"), col(\"millisinsercao\"), col(\"codclasse\"), col(\"descclasse\"), \\\n",
    "                col(\"codlocalidade\"), col(\"competencia\"), col(\"dtajuizamento\"), col(\"descsistema\"), \\\n",
    "                col(\"nivelsigilo\"), col(\"npu\"), col(\"oj_codibge\"), col(\"oj_cod\"), \\\n",
    "                col(\"oj_instancia\"), col(\"oj_descricao\"), col(\"tramitacao\"), col(\"tamanhoprocesso\"), \\\n",
    "                col(\"valorcausa\"), col(\"ass_cod\"), col(\"descricao\").alias(\"descassunto\"), col(\"ass_principal\"), col(\"ass_codlocal\"), \\\n",
    "                col(\"ass_codpainacional\"), col(\"ass_desclocal\"), col(\"mov_dtmov\"), col(\"mov_codlocal\"), \\\n",
    "                col(\"mov_codpainacional\"), col(\"mov_cod\"), col(\"descmovimento\"), col(\"mov_nivelsigilo\"), col(\"mov_oj_codibge\"), \\\n",
    "                col(\"mov_oj_cod\"), col(\"mov_oj_instancia\"), col(\"mov_oj_descricao\"), col(\"mov_tpdecisao\"), \\\n",
    "                col(\"mov_tprespmov\"), col(\"natureza\"), col(\"fase\") \\\n",
    "        )\n",
    "        \n",
    "        df_query_distinctPd = df_movimentos_join.toPandas()\n",
    "        df_query_distinctPd.to_csv('./output/movimentos_tribunais.csv', mode='a', header=is_first, sep = \";\", index=False, chunksize=1000)\n",
    "        \n",
    "        df_movimentos_join.withColumn('mov_dtmov', to_timestamp(df_movimentos_join['mov_dtmov'], 'yyyyMMddHHmmss'))\n",
    "        \n",
    "        if is_first == True:\n",
    "            is_first = False\n",
    "            df_movimentos_join.repartition(5).write \\\n",
    "                .mode(\"overwrite\") \\\n",
    "                .format(\"jdbc\") \\\n",
    "                .option(\"url\", db_url).option(\"user\", db_user).option(\"password\", db_pass).option(\"driver\", db_driver) \\\n",
    "                .option(\"dbtable\", \"inovacnj.fat_movimentos_te\") \\\n",
    "                .option(\"batchsize\", \"10000\") \\\n",
    "                .save()\n",
    "        else :\n",
    "            df_movimentos_join.repartition(5).write \\\n",
    "                .mode(\"append\") \\\n",
    "                .format(\"jdbc\") \\\n",
    "                .option(\"url\", db_url).option(\"user\", db_user).option(\"password\", db_pass).option(\"driver\", db_driver) \\\n",
    "                .option(\"dbtable\", \"inovacnj.fat_movimentos_te\") \\\n",
    "                .option(\"batchsize\", \"10000\") \\\n",
    "                .save()\n",
    "\n",
    "        print(\"Finalizando carregamento do tribunal: \" + dir_trib)\n",
    "        \n",
    "    print(\"Finalizando carregamento do ramo de justica: \" + dir_ramo_just)\n",
    "    \n",
    "print(\"Carregamento dos arquivos finalizado.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
